{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5QxONE0j0x3Ti7engl858",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakarkhanlakhwera/Deepl-Learing/blob/main/Activation_Function/Activation_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Function\n",
        "\n",
        "An **activation function** introduces non-linearity to a neural network, enabling it to learn complex relationships in data. It transforms the input signal into an output using mathematical functions like **Sigmoid**, **ReLU**, or **Tanh** to decide whether a neuron should be activated.\n"
      ],
      "metadata": {
        "id": "4SMA65iwBMQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> https://en.wikipedia.org/wiki/Activation_function\n"
      ],
      "metadata": {
        "id": "Mh6I0Xd3ES69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid Activation Function\n",
        "\n",
        "The **Sigmoid function** is defined as:  \n",
        "\\[\n",
        "f(x) = \\frac{1}{1 + e^{-x}}\n",
        "\\]\n",
        "\n",
        "It maps any input to a range between **0 and 1**, making it suitable for binary classification problems.\n",
        "\n",
        "#### ✅ **Advantages:**\n",
        "- **Smooth Gradient:** Provides a smooth gradient, preventing abrupt jumps in output.  \n",
        "- **Probabilistic Interpretation:** Output can be interpreted as probabilities.  \n",
        "- **Works Well for Binary Classification:** Suitable for problems requiring outputs between 0 and 1.  \n",
        "\n",
        "#### ❌ **Disadvantages:**\n",
        "- **Vanishing Gradient Problem:** For very large or very small inputs, gradients approach zero, slowing down training.  \n",
        "- **Not Zero-Centered:** Outputs are always positive, causing inefficient weight updates.  \n",
        "- **Computationally Expensive:** Involves exponential calculations, which can be computationally intensive.  \n"
      ],
      "metadata": {
        "id": "Lc512VGFGN_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of Activation Functions\n",
        "\n",
        "| **Activation Function** | **Mathematical Formula** | **Range** | **Gradient Type** | **Advantages** | **Disadvantages** | **Best Used In** |\n",
        "|--------------------------|--------------------------|-----------|-------------------|---------------|-------------------|-----------------|\n",
        "| **Sigmoid** | \\( f(x) = \\frac{1}{1 + e^{-x}} \\) | (0, 1) | **Vanishing Gradient** for large/small inputs | Smooth gradient, probabilistic output | Vanishing gradient, not zero-centered | Binary classification |\n",
        "| **Tanh** | \\( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\) | (-1, 1) | **Vanishing Gradient** for extreme values | Zero-centered output, stronger gradients | Vanishing gradient | Hidden layers in neural networks |\n",
        "| **ReLU (Rectified Linear Unit)** | \\( f(x) = \\max(0, x) \\) | [0, ∞) | **Non-vanishing Gradient** for positive inputs | Computationally efficient, avoids vanishing gradient | Dying ReLU problem (neurons stop activating) | Hidden layers in deep neural networks |\n",
        "| **Leaky ReLU** | \\( f(x) = \\max(\\alpha x, x) \\) | (-∞, ∞) | **Non-vanishing Gradient** for negative and positive inputs | Allows small gradients for negative values | Can result in unstable training | Addressing dying ReLU problem |\n",
        "| **Softmax** | \\( f(x_i) = \\frac{e^{x_i}}{\\sum e^{x_j}} \\) | (0, 1) (sums to 1) | **Non-vanishing Gradient** | Outputs probabilities, good for multi-class classification | Computationally expensive | Output layer in multi-class classification |\n",
        "| **ELU (Exponential Linear Unit)** | \\( f(x) = x \\) if \\( x > 0 \\), \\( f(x) = \\alpha(e^x - 1) \\) if \\( x \\leq 0 \\) | (-α, ∞) | **Non-vanishing Gradient** for all inputs | Smooth gradient, avoids vanishing gradient | Higher computational cost | Deep learning models |\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Vanishing Gradient:** Sigmoid and Tanh suffer in deeper networks.\n",
        "- **Non-vanishing Gradient:** ReLU, Leaky ReLU, Softmax, and ELU handle gradients better in deep networks.\n",
        "- **Usage:** Choose the activation function based on layer position and problem type.\n"
      ],
      "metadata": {
        "id": "7cXHPT7IJniB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISyexglq_lgL"
      },
      "outputs": [],
      "source": []
    }
  ]
}